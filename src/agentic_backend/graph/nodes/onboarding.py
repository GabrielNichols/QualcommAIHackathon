from __future__ import annotations
from typing import Dict, Any, List, Optional
import json
import asyncio
import os
from datetime import datetime
import logging

from ...llm.engine import LLMEngine
from ...embeddings.embedding import ONNXEmbedder
from ...vectorstore.faiss_store import LocalFaiss
from ...npu_monitor import npu_monitor, monitor_inference
from ...security.policies import Policy

log = logging.getLogger(__name__)


class OnboardingAgent:
    """
    Agente de Onboarding com IA real que:
    - Detecta inten√ß√£o usando an√°lise sem√¢ntica
    - Faz perguntas inteligentes usando LLM
    - Coleta contexto pessoal do usu√°rio via chat
    - Indexa informa√ß√µes no FAISS com embeddings reais
    - Permite que todos os agentes acessem o contexto
    """

    def __init__(self, llm_engine: LLMEngine, embedder: ONNXEmbedder, vector_store: LocalFaiss):
        self.llm = llm_engine
        self.embedder = embedder
        self.vector_store = vector_store
        self.conversation_history: List[Dict[str, str]] = []

        # Diret√≥rios para dados de usu√°rios
        self.users_dir = "./data/users"
        self.user_indexes_dir = "./data/user_indexes"

        # Garantir que diret√≥rios existam
        os.makedirs(self.users_dir, exist_ok=True)
        os.makedirs(self.user_indexes_dir, exist_ok=True)

        # Estado da conversa de onboarding
        self.onboarding_state = {
            "current_step": "greeting",
            "collected_info": {},
            "pending_questions": [],
            "conversation_context": ""
        }

        # Inicializar monitoramento NPU
        npu_monitor.start_monitoring()

    async def process_message(self, user_id: str, message: str) -> Dict[str, Any]:
        """
        Processa mensagem do usu√°rio e avan√ßa no onboarding usando IA real
        """
        log.info(f"üí¨ Processando mensagem do usu√°rio {user_id}: {message[:50]}...")

        # Adicionar mensagem ao hist√≥rico
        self.conversation_history.append({"role": "user", "content": message})

        # Detectar inten√ß√£o usando IA
        with monitor_inference():
            intent = self._detect_intent(message)

        # Atualizar estado da conversa
        self.onboarding_state["conversation_context"] += f"\nUsu√°rio: {message}"

        # Processar baseado na inten√ß√£o detectada
        if intent == "first_access":
            return self._start_onboarding_flow(user_id, message)
        elif intent == "continue_onboarding":
            return self._continue_onboarding_flow(user_id, message)
        elif intent == "already_onboarded":
            return self._handle_existing_user(user_id)
        else:
            return self._handle_general_query(user_id, message, intent)

    async def _detect_intent(self, message: str) -> str:
        """
        Detecta inten√ß√£o da mensagem usando an√°lise sem√¢ntica com LLM
        """
        intent_prompt = f"""
        Analise esta mensagem do usu√°rio e determine a inten√ß√£o principal:

        Mensagem: "{message}"

        Contexto da conversa anterior:
        {self.onboarding_state.get("conversation_context", "")}

        Classifique em uma das seguintes inten√ß√µes:

        1. "first_access" - Usu√°rio est√° fazendo primeiro acesso, quer come√ßar onboarding
        2. "continue_onboarding" - Usu√°rio est√° respondendo perguntas do onboarding
        3. "already_onboarded" - Usu√°rio j√° tem perfil e quer acessar funcionalidades
        4. "general_help" - Usu√°rio precisa de ajuda geral ou tem d√∫vida

        Considere palavras-chave como:
        - Primeiro acesso: "primeiro", "novo", "come√ßar", "iniciar", "ol√°", "bem-vindo"
        - J√° cadastrado: "j√° tenho", "meu perfil", "j√° fiz", "continuar"
        - Respostas: informa√ß√µes pessoais, profissionais, prefer√™ncias

        Responda apenas com a classifica√ß√£o (uma palavra).
        """

        try:
            response = self.llm.generate_text(intent_prompt, max_length=50)
            response = response.strip().lower()

            # Mapear resposta para inten√ß√µes v√°lidas
            if "first_access" in response or "primeiro" in response:
                return "first_access"
            elif "continue" in response or "resposta" in response:
                return "continue_onboarding"
            elif "already" in response or "perfil" in response:
                return "already_onboarded"
            else:
                return "general_help"

        except Exception as e:
            log.warning(f"Erro na detec√ß√£o de inten√ß√£o: {e}")
            # Fallback: analisar manualmente
            return self._fallback_intent_detection(message)

    def _fallback_intent_detection(self, message: str) -> str:
        """
        Fallback para detec√ß√£o de inten√ß√£o sem LLM
        """
        message_lower = message.lower()

        # Palavras-chave para primeiro acesso
        first_access_keywords = [
            "primeiro acesso", "primeira vez", "novo usu√°rio", "nova conta",
            "come√ßar", "iniciar", "bem vindo", "ol√°", "oi", "boa tarde",
            "n√£o tenho conta", "sou novo", "nova aqui", "novo aqui"
        ]

        # Palavras-chave para usu√°rio existente
        existing_keywords = [
            "j√° tenho", "meu perfil", "j√° fiz", "continuar", "j√° cadastrei"
        ]

        if any(keyword in message_lower for keyword in first_access_keywords):
            return "first_access"
        elif any(keyword in message_lower for keyword in existing_keywords):
            return "already_onboarded"
        else:
            return "continue_onboarding"

    async def _start_onboarding_flow(self, user_id: str, message: str) -> Dict[str, Any]:
        """
        Inicia fluxo completo de onboarding
        """
        log.info(f"üöÄ Iniciando onboarding para usu√°rio: {user_id}")

        # Verificar se j√° existe perfil
        if self._user_exists(user_id):
            # Oferecer carregar perfil existente ou recriar
            return self._handle_existing_profile_choice(user_id)

        # Resetar estado do onboarding
        self.onboarding_state = {
            "current_step": "greeting",
            "collected_info": {},
            "pending_questions": [],
            "conversation_context": f"Usu√°rio iniciou onboarding: {message}"
        }

        # Iniciar com sauda√ß√£o
        greeting_response = self._generate_greeting_response(user_id)

        return {
            "response": greeting_response,
            "onboarding_status": "started",
            "next_step": "ask_name",
            "user_id": user_id,
            "requires_user_input": True
        }

    async def _continue_onboarding_flow(self, user_id: str, message: str) -> Dict[str, Any]:
        """
        Continua o fluxo de onboarding baseado no estado atual
        """
        current_step = self.onboarding_state.get("current_step", "greeting")

        # Processar resposta baseado na etapa atual
        if current_step == "ask_name":
            return self._process_name_response(user_id, message)
        elif current_step == "ask_profession":
            return self._process_profession_response(user_id, message)
        elif current_step == "ask_preferences":
            return self._process_preferences_response(user_id, message)
        elif current_step == "complete":
            return self._finalize_onboarding(user_id)
        else:
            # Detectar automaticamente o que perguntar
            return self._generate_next_question(user_id, message)

    async def _handle_existing_user(self, user_id: str) -> Dict[str, Any]:
        """
        Trata usu√°rio que j√° tem perfil
        """
        if self._user_exists(user_id):
            user_context = self._load_user_context(user_id)
            return {
                "response": f"Ol√°! Bem-vindo de volta. Seu perfil j√° est√° configurado.",
                "user_context": user_context,
                "onboarding_status": "already_completed",
                "user_id": user_id
            }
        else:
            return await self._start_onboarding_flow(user_id, "Ol√°")

    async def _handle_general_query(self, user_id: str, message: str, intent: str) -> Dict[str, Any]:
        """
        Trata consultas gerais durante ou fora do onboarding
        """
        # Verificar se estamos no meio do onboarding
        if self.onboarding_state.get("current_step") not in ["greeting", "complete"]:
            # Estamos no meio do onboarding, processar como resposta
            return await self._continue_onboarding_flow(user_id, message)

        # Consulta geral - tentar detectar se √© sobre onboarding
        if "onboarding" in message.lower() or "cadastro" in message.lower():
            return await self._start_onboarding_flow(user_id, message)

        # Resposta gen√©rica
        return {
            "response": "Ol√°! Sou o assistente de integra√ß√£o Ita√∫. Posso ajudar voc√™ com o processo de cadastro ou acessar suas funcionalidades j√° configuradas.",
            "onboarding_status": "general_help",
            "user_id": user_id
        }

    async def _generate_greeting_response(self, user_id: str) -> str:
        """
        Gera resposta de sauda√ß√£o personalizada usando LLM
        """
        greeting_prompt = f"""
        Voc√™ √© o assistente de integra√ß√£o do Ita√∫. Um usu√°rio (ID: {user_id})
        est√° fazendo seu primeiro acesso ao sistema.

        Crie uma sauda√ß√£o acolhedora e profissional que:
        1. Demonstre conhecimento do contexto Ita√∫
        2. Explique brevemente o prop√≥sito do onboarding
        3. Incentive o usu√°rio a fornecer informa√ß√µes
        4. Mantenha tom amig√°vel mas profissional

        A sauda√ß√£o deve preparar o usu√°rio para responder perguntas sobre:
        - Informa√ß√µes pessoais b√°sicas
        - Perfil profissional
        - Prefer√™ncias de uso

        M√°ximo 150 palavras.
        """

        greeting = self.llm.generate_text(greeting_prompt, max_length=150)

        # Adicionar ao hist√≥rico
        self.conversation_history.append({"role": "assistant", "content": greeting})

        return greeting.strip()

    async def _generate_next_question(self, user_id: str, user_response: str) -> Dict[str, Any]:
        """
        Gera pr√≥xima pergunta inteligente baseada na resposta anterior usando LLM
        """
        # Analisar informa√ß√µes j√° coletadas
        collected_info = self.onboarding_state.get("collected_info", {})

        question_prompt = f"""
        Voc√™ √© especialista em onboarding Ita√∫. Baseado nas informa√ß√µes j√° coletadas
        e na √∫ltima resposta do usu√°rio, determine qual √© a pr√≥xima pergunta mais
        relevante e √∫til para fazer.

        Informa√ß√µes j√° coletadas:
        {json.dumps(collected_info, indent=2, ensure_ascii=False)}

        √öltima resposta do usu√°rio: "{user_response}"

        Contexto da conversa:
        {self.onboarding_state.get("conversation_context", "")}

        Decida qual informa√ß√£o ainda precisamos coletar:
        1. Se ainda n√£o temos nome ‚Üí perguntar nome
        2. Se ainda n√£o temos informa√ß√µes profissionais ‚Üí perguntar cargo/√°rea
        3. Se ainda n√£o temos prefer√™ncias ‚Üí perguntar sites/ferramentas
        4. Se temos informa√ß√µes b√°sicas ‚Üí perguntar sobre experi√™ncia espec√≠fica
        5. Se temos tudo b√°sico ‚Üí finalizar com pergunta sobre objetivos

        Gere uma pergunta natural e contextualizada que ajude a personalizar
        a experi√™ncia Ita√∫ do usu√°rio.

        Responda apenas com a pergunta, sem explica√ß√µes adicionais.
        """

        try:
            next_question = self.llm.generate_text(question_prompt, max_length=100)

            # Identificar tipo da pergunta para atualizar estado
            question_type = self._classify_question_type(next_question, collected_info)

            # Atualizar estado
            self.onboarding_state["current_step"] = question_type
            self.onboarding_state["last_question"] = next_question

            # Adicionar pergunta ao hist√≥rico
            self.conversation_history.append({"role": "assistant", "content": next_question})

            return {
                "response": next_question.strip(),
                "onboarding_status": "in_progress",
                "question_type": question_type,
                "user_id": user_id,
                "requires_user_input": True
            }

        except Exception as e:
            log.error(f"Erro ao gerar pr√≥xima pergunta: {e}")
            return await self._fallback_question_generation(user_id, collected_info)

    async def _classify_question_type(self, question: str, collected_info: Dict) -> str:
        """
        Classifica o tipo da pergunta gerada usando LLM
        """
        classify_prompt = f"""
        Classifique esta pergunta de onboarding em uma categoria:

        Pergunta: "{question}"

        Informa√ß√µes j√° coletadas: {list(collected_info.keys())}

        Categorias:
        - "ask_name" - Pergunta sobre nome ou identifica√ß√£o
        - "ask_personal" - Pergunta sobre informa√ß√µes pessoais (idade, localiza√ß√£o)
        - "ask_profession" - Pergunta sobre cargo, √°rea, experi√™ncia profissional
        - "ask_experience" - Pergunta sobre experi√™ncia espec√≠fica no Ita√∫
        - "ask_preferences" - Pergunta sobre prefer√™ncias de uso, sites, ferramentas
        - "ask_goals" - Pergunta sobre objetivos ou necessidades espec√≠ficas
        - "finalize" - Momento de finalizar o onboarding

        Responda apenas com a categoria.
        """

        try:
            response = self.llm.generate_text(classify_prompt, max_length=30)
            category = response.strip().lower()

            # Mapear para categorias v√°lidas
            valid_categories = [
                "ask_name", "ask_personal", "ask_profession",
                "ask_experience", "ask_preferences", "ask_goals", "finalize"
            ]

            for cat in valid_categories:
                if cat in category:
                    return cat

            return "ask_general"

        except Exception as e:
            log.warning(f"Erro na classifica√ß√£o: {e}")
            return "ask_general"

    def _fallback_question_generation(self, user_id: str, collected_info: Dict) -> Dict[str, Any]:
        """
        Gera√ß√£o de pergunta fallback quando LLM falha
        """
        questions = []

        if "nome" not in collected_info:
            questions.append(("Qual seu nome completo?", "ask_name"))
        elif "cargo" not in collected_info:
            questions.append(("Qual seu cargo atual no Ita√∫?", "ask_profession"))
        elif "area" not in collected_info:
            questions.append(("Em qual √°rea voc√™ trabalha?", "ask_profession"))
        elif "sites_frequentes" not in collected_info:
            questions.append(("Quais sites/portais voc√™ mais acessa no trabalho?", "ask_preferences"))
        else:
            questions.append(("Qual seu objetivo principal ao usar o sistema Ita√∫?", "ask_goals"))

        question, qtype = questions[0]

        return {
            "response": question,
            "onboarding_status": "in_progress",
            "question_type": qtype,
            "user_id": user_id,
            "requires_user_input": True
        }

    async def _handle_existing_profile_choice(self, user_id: str) -> Dict[str, Any]:
        """
        Trata escolha do usu√°rio sobre perfil existente
        """
        return {
            "response": f"Encontrei um perfil existente para voc√™ ({user_id}). Deseja carregar o perfil existente ou criar um novo?",
            "onboarding_status": "profile_choice",
            "user_id": user_id,
            "requires_user_input": True,
            "options": ["carregar", "novo"]
        }

    async def _process_name_response(self, user_id: str, message: str) -> Dict[str, Any]:
        """
        Processa resposta sobre nome usando LLM para extrair informa√ß√£o
        """
        extract_prompt = f"""
        Extraia o nome completo da seguinte mensagem do usu√°rio:

        Mensagem: "{message}"

        Se houver um nome completo, responda apenas com o nome.
        Se n√£o houver nome claro, responda com "NOME_NAO_ENCONTRADO".
        """

        try:
            extracted_name = self.llm.generate_text(extract_prompt, max_length=50)
            extracted_name = extracted_name.strip()

            if "NOME_NAO_ENCONTRADO" in extracted_name:
                # Pedir novamente
                return self._ask_for_clarification(user_id, "nome", message)

            # Salvar nome
            self.onboarding_state["collected_info"]["nome"] = extracted_name
            self.onboarding_state["conversation_context"] += f"\nNome identificado: {extracted_name}"

            # Pr√≥xima pergunta
            return self._generate_next_question(user_id, message)

        except Exception as e:
            log.error(f"Erro ao processar resposta de nome: {e}")
            return await self._generate_next_question(user_id, message)

    async def _process_profession_response(self, user_id: str, message: str) -> Dict[str, Any]:
        """
        Processa resposta sobre informa√ß√µes profissionais
        """
        extract_prompt = f"""
        Analise esta resposta sobre informa√ß√µes profissionais e extraia:
        - Cargo
        - √Årea de atua√ß√£o
        - Anos de experi√™ncia
        - Principais atividades

        Resposta do usu√°rio: "{message}"

        Formate como JSON v√°lido com as chaves: cargo, area, experiencia_anos, atividades.
        Se alguma informa√ß√£o n√£o estiver clara, use "NAO_INFORMADO".
        """

        try:
            response = self.llm.generate_text(extract_prompt, max_length=200)

            # Tentar extrair JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                prof_info = json.loads(json_match.group())

                # Salvar informa√ß√µes
                collected = self.onboarding_state["collected_info"]
                collected.update(prof_info)
                self.onboarding_state["conversation_context"] += f"\nInforma√ß√µes profissionais: {prof_info}"

                # Pr√≥xima pergunta
                return await self._generate_next_question(user_id, message)
            else:
                return await self._ask_for_clarification(user_id, "profissional", message)

        except Exception as e:
            log.error(f"Erro ao processar resposta profissional: {e}")
            return await self._generate_next_question(user_id, message)

    async def _process_preferences_response(self, user_id: str, message: str) -> Dict[str, Any]:
        """
        Processa resposta sobre prefer√™ncias
        """
        extract_prompt = f"""
        Analise esta resposta sobre prefer√™ncias de uso e extraia:
        - Sites/portais frequentes
        - Ferramentas favoritas
        - Tipo de conte√∫do preferido
        - Formato preferido
        - Hor√°rio de pico

        Resposta do usu√°rio: "{message}"

        Formate como JSON v√°lido.
        """

        try:
            response = self.llm.generate_text(extract_prompt, max_length=200)

            # Tentar extrair JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                pref_info = json.loads(json_match.group())

                # Salvar informa√ß√µes
                collected = self.onboarding_state["collected_info"]
                collected.update(pref_info)
                self.onboarding_state["conversation_context"] += f"\nPrefer√™ncias: {pref_info}"

                # Pr√≥xima pergunta ou finalizar
                return await self._check_completion_and_finalize(user_id)
            else:
                return await self._ask_for_clarification(user_id, "prefer√™ncias", message)

        except Exception as e:
            log.error(f"Erro ao processar resposta de prefer√™ncias: {e}")
            return await self._check_completion_and_finalize(user_id)

    async def _ask_for_clarification(self, user_id: str, topic: str, original_message: str) -> Dict[str, Any]:
        """
        Pede esclarecimento quando resposta n√£o est√° clara
        """
        clarification_prompt = f"""
        O usu√°rio deu uma resposta que n√£o est√° clara sobre {topic}.
        Resposta original: "{original_message}"

        Gere uma pergunta de esclarecimento amig√°vel e espec√≠fica
        que ajude o usu√°rio a fornecer a informa√ß√£o necess√°ria.
        """

        try:
            clarification = self.llm.generate_text(clarification_prompt, max_length=100)

            return {
                "response": clarification.strip(),
                "onboarding_status": "clarification_needed",
                "clarification_topic": topic,
                "user_id": user_id,
                "requires_user_input": True
            }
        except Exception as e:
            return {
                "response": f"Poderia esclarecer melhor sobre {topic}?",
                "onboarding_status": "clarification_needed",
                "user_id": user_id,
                "requires_user_input": True
            }

    async def _check_completion_and_finalize(self, user_id: str) -> Dict[str, Any]:
        """
        Verifica se temos informa√ß√µes suficientes para finalizar
        """
        collected = self.onboarding_state["collected_info"]

        # Verificar informa√ß√µes essenciais
        essential_fields = ["nome", "cargo", "area"]
        missing_fields = [field for field in essential_fields if field not in collected]

        if missing_fields:
            # Ainda falta informa√ß√£o essencial
            return await self._generate_next_question(user_id, "continuar")

        # Temos informa√ß√µes suficientes - finalizar
        return await self._finalize_onboarding(user_id)

    async def _finalize_onboarding(self, user_id: str) -> Dict[str, Any]:
        """
        Finaliza o processo de onboarding
        """
        log.info(f"üéâ Finalizando onboarding para {user_id}")

        # Criar perfil completo
        profile = self._create_complete_profile(user_id)

        # Salvar perfil
        self._save_user_profile(profile)

        # Indexar no FAISS
        self._index_user_profile(profile)

        # Preparar contexto para outros agentes
        user_context = self._prepare_user_context(profile)

        # Mensagem de conclus√£o
        completion_message = self._generate_completion_message(profile)

        return {
            "response": completion_message,
            "onboarding_status": "completed",
            "user_id": user_id,
            "user_context": user_context,
            "user_profile": profile,
            "requires_user_input": False
        }

    async def _create_complete_profile(self, user_id: str) -> Dict[str, Any]:
        """
        Cria perfil completo baseado nas informa√ß√µes coletadas
        """
        collected = self.onboarding_state["collected_info"]

        # Organizar em estrutura padr√£o
        profile = {
            "user_id": user_id,
            "personal_info": {
                "nome": collected.get("nome", ""),
                "idade": collected.get("idade", ""),
                "localizacao": collected.get("localizacao", ""),
                "experiencia_bancaria": collected.get("experiencia_bancaria", "")
            },
            "professional_info": {
                "cargo": collected.get("cargo", ""),
                "area": collected.get("area", ""),
                "experiencia_anos": collected.get("experiencia_anos", ""),
                "principais_atividades": collected.get("principais_atividades", ""),
                "nivel_acesso": collected.get("nivel_acesso", "")
            },
            "preferences": {
                "sites_frequentes": collected.get("sites_frequentes", ""),
                "ferramentas_favoritas": collected.get("ferramentas_favoritas", ""),
                "tipo_conteudo": collected.get("tipo_conteudo", ""),
                "formato_preferido": collected.get("formato_preferido", ""),
                "horario_pico": collected.get("horario_pico", "")
            },
            "usage_patterns": self._estimate_usage_patterns(
                profile["professional_info"],
                profile["preferences"]
            ),
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "onboarding_version": "3.0",  # Vers√£o com IA real completa
            "ai_generated": True,
            "conversation_history": self.conversation_history
        }

        return profile

    async def _estimate_usage_patterns(self, professional_info: Dict, preferences: Dict) -> Dict[str, Any]:
        """Estima padr√µes de uso usando LLM"""
        patterns_prompt = f"""
        Voc√™ √© analista de comportamento de usu√°rios Ita√∫.
        Com base neste perfil profissional:

        Cargo: {professional_info["cargo"]}
        √Årea: {professional_info["area"]}
        Experi√™ncia: {professional_info["experiencia_anos"]} anos
        Atividades: {professional_info["principais_atividades"]}
        Hor√°rio pico: {preferences["horario_pico"]}

        Estime padr√µes de uso t√≠picos incluindo:
        - Frequ√™ncia de uso
        - Tipos de opera√ß√µes realizadas
        - Prioridades no trabalho
        - Hor√°rios de atividade

        Formate como JSON v√°lido.
        """

        try:
            patterns_text = self.llm.generate_text(patterns_prompt, max_length=400)

            # Tentar extrair JSON da resposta
            import re
            json_match = re.search(r'\{.*\}', patterns_text, re.DOTALL)
            if json_match:
                patterns = json.loads(json_match.group())
            else:
                # Fallback
                patterns = {
                    "frequencia_uso": "diario",
                    "tipo_operacoes": ["pesquisa_regulatoria"],
                    "prioridades": ["conformidade"],
                    "horarios_atividade": ["09:00-18:00"]
                }

        except Exception as e:
            log.warning(f"Erro ao estimar padr√µes: {e}")
            patterns = {
                "frequencia_uso": "diario",
                "tipo_operacoes": ["pesquisa_regulatoria"],
                "prioridades": ["conformidade"],
                "horarios_atividade": ["09:00-18:00"]
            }

        return patterns

    async def _generate_completion_message(self, profile: Dict) -> str:
        """
        Gera mensagem de conclus√£o personalizada
        """
        completion_prompt = f"""
        Voc√™ √© assistente Ita√∫. O usu√°rio {profile["personal_info"]["nome"]}
        ({profile["professional_info"]["cargo"]} da √°rea {profile["professional_info"]["area"]})
        acabou de completar o onboarding.

        Crie uma mensagem de conclus√£o que:
        1. Agrade√ßa pela participa√ß√£o
        2. Destaque que o sistema est√° personalizado
        3. Mencione pr√≥ximos passos
        4. Incentive o uso das funcionalidades

        Mantenha tom profissional e acolhedor.
        M√°ximo 200 palavras.
        """

        completion = self.llm.generate_text(completion_prompt, max_length=200)
        return completion.strip()

    async def _save_user_profile(self, profile: Dict[str, Any]):
        """Salva perfil do usu√°rio em arquivo JSON"""
        user_file = f"{self.users_dir}/{profile['user_id']}.json"

        with open(user_file, 'w', encoding='utf-8') as f:
            json.dump(profile, f, ensure_ascii=False, indent=2)

        log.info(f"üíæ Perfil salvo: {user_file}")

    async def _index_user_profile(self, profile: Dict[str, Any]):
        """Indexa perfil do usu√°rio no FAISS usando embeddings reais"""
        user_id = profile["user_id"]

        # Preparar documentos para indexa√ß√£o
        documents = self._prepare_documents_for_indexing(profile)

        # Indexar cada documento
        for doc in documents:
            try:
                # Gerar embedding real do conte√∫do
                content = doc["content"]
                embedding = self.embedder.embed([content])[0]

                # Adicionar ao vector store com metadata
                metadata = {
                    "user_id": user_id,
                    "doc_type": doc["type"],
                    "category": doc["category"],
                    "timestamp": datetime.now().isoformat()
                }

                # Adicionar ao FAISS
                self.vector_store.add(embedding.reshape(1, -1), [content], [metadata])

                log.info(f"‚úÖ Documento indexado: {doc['type']}")

            except Exception as e:
                log.error(f"‚ùå Erro ao indexar {doc['type']}: {e}")

    def _prepare_documents_for_indexing(self, profile: Dict) -> List[Dict[str, Any]]:
        """Prepara documentos estruturados para indexa√ß√£o"""
        documents = []

        # 1. Documento de informa√ß√µes pessoais
        personal_doc = {
            "type": "personal_info",
            "category": "personal",
            "content": f"""
            Informa√ß√µes pessoais de {profile["personal_info"]["nome"]}:
            - Nome: {profile["personal_info"]["nome"]}
            - Idade: {profile["personal_info"]["idade"]}
            - Localiza√ß√£o: {profile["personal_info"]["localizacao"]}
            - Experi√™ncia banc√°ria: {profile["personal_info"]["experiencia_bancaria"]}
            """
        }
        documents.append(personal_doc)

        # 2. Documento de informa√ß√µes profissionais
        prof_doc = {
            "type": "professional_info",
            "category": "professional",
            "content": f"""
            Informa√ß√µes profissionais de {profile["personal_info"]["nome"]}:
            - Cargo: {profile["professional_info"]["cargo"]}
            - √Årea: {profile["professional_info"]["area"]}
            - Experi√™ncia: {profile["professional_info"]["experiencia_anos"]} anos
            - Atividades principais: {profile["professional_info"]["principais_atividades"]}
            - N√≠vel de acesso: {profile["professional_info"]["nivel_acesso"]}
            """
        }
        documents.append(prof_doc)

        # 3. Documento de prefer√™ncias
        pref_doc = {
            "type": "preferences",
            "category": "preferences",
            "content": f"""
            Prefer√™ncias de uso de {profile["personal_info"]["nome"]}:
            - Sites frequentes: {profile["preferences"]["sites_frequentes"]}
            - Ferramentas favoritas: {profile["preferences"]["ferramentas_favoritas"]}
            - Tipo de conte√∫do: {profile["preferences"]["tipo_conteudo"]}
            - Formato preferido: {profile["preferences"]["formato_preferido"]}
            - Hor√°rio de pico: {profile["preferences"]["horario_pico"]}
            """
        }
        documents.append(pref_doc)

        return documents

    async def _prepare_user_context(self, profile: Dict) -> Dict[str, Any]:
        """Prepara contexto do usu√°rio para outros agentes"""
        return {
            "user_id": profile["user_id"],
            "personal_info": profile["personal_info"],
            "professional_info": profile["professional_info"],
            "preferences": profile["preferences"],
            "usage_patterns": profile["usage_patterns"],
            "context_summary": self._generate_context_summary(profile),
            "last_updated": profile["updated_at"]
        }

    async def _generate_context_summary(self, profile: Dict) -> str:
        """Gera resumo inteligente do contexto usando LLM"""
        summary_prompt = f"""
        Voc√™ √© assistente Ita√∫ especializado em an√°lise de perfis de usu√°rio.
        Crie um resumo executivo conciso do perfil deste usu√°rio:

        NOME: {profile["personal_info"]["nome"]}
        CARGO: {profile["professional_info"]["cargo"]}
        √ÅREA: {profile["professional_info"]["area"]}
        EXPERI√äNCIA: {profile["professional_info"]["experiencia_anos"]} anos
        PREFER√äNCIAS: {profile["preferences"]["tipo_conteudo"]}

        O resumo deve ser √∫til para outros agentes personalizarem atendimento.
        M√°ximo 200 caracteres.
        """

        summary = self.llm.generate_text(summary_prompt, max_length=100)
        return summary.strip()

    async def _user_exists(self, user_id: str) -> bool:
        """Verifica se usu√°rio j√° tem perfil salvo"""
        user_file = f"{self.users_dir}/{user_id}.json"
        return os.path.exists(user_file)

    async def _load_user_context(self, user_id: str) -> Dict[str, Any]:
        """Carrega contexto do usu√°rio do arquivo"""
        user_file = f"{self.users_dir}/{user_id}.json"

        if not os.path.exists(user_file):
            return {}

        with open(user_file, 'r', encoding='utf-8') as f:
            profile = json.load(f)

        return await self._prepare_user_context(profile)

    async def get_user_context_for_agents(self, user_id: str, query: str = "") -> Dict[str, Any]:
        """
        M√©todo principal para outros agentes obterem contexto do usu√°rio.
        Inclui busca inteligente no RAG se houver query.
        """
        if not self._user_exists(user_id):
            return {"error": "Usu√°rio n√£o encontrado"}

        # Carregar perfil b√°sico
        context = self._load_user_context(user_id)

        # Se h√° query, fazer busca inteligente no RAG
        if query and self.vector_store:
            try:
                # Gerar embedding da query
                query_embedding = self.embedder.embed([query])[0]

                # Buscar documentos relevantes do usu√°rio
                results = self.vector_store.search(query_embedding.reshape(1, -1), k=5)

                # Filtrar apenas documentos deste usu√°rio
                user_docs = []
                for doc, score in results:
                    # Verificar se documento pertence ao usu√°rio (simula√ß√£o)
                    if user_id in doc.lower():
                        user_docs.append({"content": doc, "score": score})

                context["relevant_docs"] = user_docs[:3]  # Top 3 mais relevantes

            except Exception as e:
                log.warning(f"Erro na busca RAG: {e}")

        return context


async def run(state: Dict[str, Any], llm_engine: LLMEngine, embedder: ONNXEmbedder) -> Dict[str, Any]:
    """
    Fun√ß√£o principal do Onboarding Agent - ponto de entrada para o graph
    """
    # Inicializar componentes de IA
    vector_store = LocalFaiss(dim=768, index_dir="./data/user_indexes")

    # Criar agente de onboarding
    onboarding_agent = OnboardingAgent(llm_engine, embedder, vector_store)

    # Processar mensagem do usu√°rio
    user_id = state.get("user_id", "unknown_user")
    message = state.get("message", "")

    if not message and not state.get("first_access", False):
        return {
            "response": "Ol√°! Como posso ajudar voc√™ hoje?",
            "onboarding_status": "general_help",
            "user_id": user_id
        }

    # Processar a mensagem
    result = await onboarding_agent.process_message(user_id, message)

    return result
