from __future__ import annotations
from typing import Dict, Any, List
from ...tools.mcp_client import MCPClient
from ...security.policies import Policy
from ...audit.evidence import EvidencePack
from ...llm.engine import LLMEngine
from ...embeddings.embedding import ONNXEmbedder
from ...vectorstore.faiss_store import LocalFaiss
import asyncio
import logging

log = logging.getLogger(__name__)

async def run(state: Dict[str, Any], llm_engine: LLMEngine, embedder: ONNXEmbedder, vector_store: LocalFaiss = None) -> Dict[str, Any]:
    """
    Researcher que usa IA real para pesquisa inteligente com RAG.

    Args:
        state: Estado atual da conversa
        llm_engine: Engine de LLM para geraÃ§Ã£o de respostas
        embedder: Embedder para gerar embeddings
        vector_store: Vector store para busca por similaridade (opcional)

    Returns:
        Estado atualizado com resposta do researcher
    """
    query = state.get("query", "")
    plan = [f"Pesquisar fontes para: {query}", "Usar RAG para encontrar contexto", "Gerar resposta com LLM"]
    state["plan"] = plan

    print(f"ğŸ” RESEARCHER: Recebendo query: '{query}'")

    try:
        # PASSO 1: Usar RAG para encontrar contexto relevante
        print("ğŸ“š PASSO 1: Buscando contexto no RAG...")

        rag_context = ""
        if embedder and vector_store:
            # Gerar embedding da query
            query_embedding = embedder.embed([query])[0]
            print(f"ğŸ”§ Embedding gerado: shape={query_embedding.shape}")

            # Buscar documentos similares
            search_results = vector_store.search(query_embedding.reshape(1, -1), k=5)
            print(f"ğŸ” Busca FAISS encontrou {len(search_results)} resultados")

            # Construir contexto dos documentos encontrados
            relevant_docs = []
            for doc_text, score in search_results:
                if score > 0.3:  # Threshold de similaridade
                    relevant_docs.append(doc_text)
                    print(f"ğŸ“„ Documento relevante (score: {score:.3f}): {doc_text[:100]}...")

            if relevant_docs:
                rag_context = "\n".join(relevant_docs)
                print(f"âœ… Contexto RAG construÃ­do: {len(rag_context)} caracteres")
            else:
                print("âš ï¸ Nenhum documento relevante encontrado no RAG")
        else:
            print("âš ï¸ RAG nÃ£o disponÃ­vel (embedder ou vector_store nÃ£o fornecidos)")

        # PASSO 2: Usar LLM para gerar resposta baseada no contexto
        print("ğŸ¤– PASSO 2: Gerando resposta com LLM...")

        if rag_context:
            # Tem contexto do RAG - usar para resposta informada
            research_prompt = f"""
VocÃª Ã© um pesquisador especialista em ItaÃº e bancos brasileiros.
Baseando-se nestas informaÃ§Ãµes encontradas:

{rag_context}

Responda Ã  pergunta: {query}

ForneÃ§a uma resposta precisa, Ãºtil e baseada nas informaÃ§Ãµes fornecidas.
Se as informaÃ§Ãµes nÃ£o forem suficientes, diga claramente.
"""
        else:
            # Sem contexto - resposta baseada em conhecimento geral
            research_prompt = f"""
VocÃª Ã© um pesquisador especialista em ItaÃº e bancos brasileiros.

Pergunta: {query}

ForneÃ§a uma resposta baseada em seu conhecimento sobre o ItaÃº.
Seja preciso e Ãºtil.
"""

        print(f"ğŸ“ Prompt para LLM: {research_prompt[:200]}...")

        # Gerar resposta usando LLM
        final_response = await asyncio.to_thread(
            llm_engine.generate_text,
            research_prompt,
            max_length=300,
            system_prompt="VocÃª Ã© um assistente especializado em informaÃ§Ãµes sobre ItaÃº e serviÃ§os bancÃ¡rios."
        )

        print(f"âœ… Resposta gerada: {final_response[:200]}...")

        # PASSO 3: Atualizar estado
        state["response"] = final_response
        state["researcher_response"] = final_response
        state["rag_context_used"] = rag_context
        state["research_completed"] = True

        print("ğŸ‰ RESEARCHER: Pesquisa concluÃ­da com sucesso!")

    except Exception as e:
        log.error(f"âŒ Erro no researcher: {e}")
        print(f"âŒ ERRO no researcher: {e}")

        # Fallback se algo falhar
        state["response"] = f"Desculpe, nÃ£o foi possÃ­vel completar a pesquisa sobre: {query}"
        state["researcher_response"] = f"Erro na pesquisa: {str(e)}"
        state["research_completed"] = False

    return state
