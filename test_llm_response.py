#!/usr/bin/env python3

import requests
import time

print('ğŸ§ª TESTANDO RESPOSTA DA LLM LOCAL')

base_url = 'http://localhost:8080'

# Testes de mensagens
test_messages = [
    "OlÃ¡, como vocÃª estÃ¡?",
    "Qual Ã© a capital do Brasil?",
    "Explique brevemente o que Ã© machine learning",
    "Conte uma piada curta",
    "O que vocÃª sabe sobre o ItaÃº?"
]

for i, message in enumerate(test_messages, 1):
    print(f'\n{i}ï¸âƒ£ Teste: "{message}"')

    payload = {
        'message': message,
        'conversation_id': f'test_llm_{i}'
    }

    try:
        start_time = time.time()
        response = requests.post(f'{base_url}/chat', json=payload, timeout=30)
        end_time = time.time()

        if response.status_code == 200:
            data = response.json()
            print(f'âœ… Status: {response.status_code}')
            print(f'â±ï¸ Tempo total: {end_time - start_time:.2f}s')
            print(f'ğŸ¤– Resposta da LLM: {data["response"][:200]}...')

            if len(data["response"]) > 200:
                print(f'ğŸ“ ContinuaÃ§Ã£o: ...{data["response"][200:400]}')

            print(f'ğŸ“Š Processing time: {data["processing_time_seconds"]:.2f}s')
            print(f'ğŸ†” Conversation ID: {data["conversation_id"]}')

        else:
            print(f'âŒ Erro: {response.status_code}')
            print(f'ğŸ“ Resposta: {response.text}')

    except Exception as e:
        print(f'âŒ Erro na requisiÃ§Ã£o: {e}')

    print('â”€' * 50)
    time.sleep(2)  # Pequena pausa entre testes

print('\nğŸ‰ TESTES DA LLM LOCAL CONCLUÃDOS!')
print('âœ… LLM Engine: Carregado e funcionando')
print('âœ… Respostas: Geradas pela IA local')
print('âœ… Performance: Tempos de resposta registrados')
